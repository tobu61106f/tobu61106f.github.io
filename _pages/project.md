---
layout: archive
title: "Project"
permalink: /project/
author_profile: true
redirect_from:
  - /research
---

# Scientific Machine Learning/Machine Learning for Science (AI4Science)
## Convex Importance-Weight Optimization for Diffusion Model Training Timesteps
**Convex Optimization Course Project**, Author: *Ziqing Chang*, *Zhiyun Yu*

The Problem: 
Standard DDPM training uses "uniform" weights ğ‘¤_ğ‘¡=1\/ğ‘‡ or â€cosine" weights , which ignores that different timesteps ğ‘¡ have different loss magnitudes and variances.

Our Method: 
The Convex Formulation (P) We find the optimal weights ğ‘¤^âˆ— by solving a strictly convex quadratic program (QP): 
\[
\min_{w \in \Delta^T} F(w)
= \sum_{t=1}^{T} w_t \hat{l}_t
+ \lambda \sum_{t=1}^{T} w_t^2 \sigma_t^2
\]
We tranning the Baseline Model to collect empirical data (ğ‘™_t )Â Ì‚ğ‘ğ‘›ğ‘‘ ğœ_ğ‘¡^2 on the MNIST Set, later we will use KKT conditions to find the closed-form solution on primal model for ğ‘¤^âˆ—. In final stage we evaluated and compared Model_Baseline and Model_Optimal, the KKT-derived w* minimizes the surrogate objective FÎ»(w), and in practice this translates into better optimization behavior during training.


---
# IC Design

---

